{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74c63c6c-5393-4200-b3ae-91b54579819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da86390d-d275-44ad-aa6a-b531244720dc",
   "metadata": {},
   "source": [
    "Итоговый сервис будет тестироваться через REST API тестовыми кейсами. Требования к REST API для участников таковы:\n",
    "\n",
    "1. REST API должен принимать данные в виде следующего JSON формата: {\"question\": \"Как изменить пароль?\"}.\n",
    "2. В ответ мы ожидаем данные в JSON формате: {\"answer\": \"Какой-то ответ\", \"class_1\": \"some_class\", \"class_2\": \"some_class\"}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5334fd6e-900e-41fe-a661-4aca900fc913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75029538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "def load_faiss_index():\n",
    "    with open('./Model/chunks_data.pickle', 'rb') as handle: #Model/\n",
    "        chunks = pickle.load(handle)\n",
    "\n",
    "    model = SentenceTransformer('BAAI/bge-m3', device=\"cpu\")\n",
    "    embeddings = np.load('./Model/embs-data.npy')#Model/\n",
    "\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])  # build the index\n",
    "    index.add(embeddings)\n",
    "\n",
    "    return chunks, model, index, embeddings\n",
    "\n",
    "\n",
    "def get_relevant_documents(question, chunks, model, index, embeddings):\n",
    "    k = 3  # NUM of retrieval candidates\n",
    "    e = model.encode(question)\n",
    "    dist, Idx = index.search(e.reshape(1, -1), k)\n",
    "    retrievals = [chunks[i] for i in Idx.flatten()]\n",
    "    return retrievals\n",
    "\n",
    "def respond_question(prompt):\n",
    "    if prompt:\n",
    "        # Check which model is selected and call the corresponding function\n",
    "        chunks, model, index, embeddings = load_faiss_index()\n",
    "        response, metadatas, rets = generate_mixtral_response(prompt, chunks, model, index,\n",
    "                                                              embeddings)  # generate_mixtral_comment(question, answer, chunks, model, index, embeddings)#generate_mixtral_response(prompt, chunks, model, index, embeddings)\n",
    "        # Process and display the response\n",
    "        excerpts = '\\n\\n'.join([f\"ИСТОЧНИК {i + 1}, {list(ret.metadata.keys())[0]}: {ret.page_content}\" for i, ret in\n",
    "                                enumerate(rets)]) if rets else \"\"\n",
    "        bibliography = '\\n\\n'.join([f\"{i + 1}. {meta}\" for i, meta in enumerate(metadatas)]) if metadatas else \"\"\n",
    "        full_response = f\"{response}\\n\\n\\n\\nИСТОЧНИКИ:\\n{excerpts}\\n\\nСПИСОК ЛИТЕРАТУРЫ:\\n\\n{bibliography}\"\n",
    "    return full_response  # f\"{response}#\\n\\nИСТОЧНИКИ:\\n{excerpts}\\n\\nСПИСОК ЛИТЕРАТУРЫ:\\n\\n{bibliography}\"#full_response\n",
    "\n",
    "def generate_mixtral_response(question, chunks, model, index, embeddings):\n",
    "    rets = get_relevant_documents(question, chunks, model, index, embeddings)\n",
    "    metadatas = [list(item.metadata.keys())[0] for item in rets]\n",
    "\n",
    "    # Improved string formatting\n",
    "    sources_text = ' \\n\\n '.join([f'ИСТОЧНИК {i + 1}: {ret.page_content}' for i, ret in enumerate(rets)])\n",
    "\n",
    "    promptstring = (\n",
    "        f\"Вы оператор поддержки, который вежливо отвечает на вопросы пользователей на темы, связанные с видеохостингом. \"\n",
    "        f\"Используя только информацию, содержащуюся в ИСТОЧНИКАХ после слова ТЕКСТ, \"\n",
    "        f\"ответьте на вопрос, заданный после слова ВОПРОС. \"\n",
    "        f\"Если в тексте нет информации, необходимой для ответа, ответьте «Недостаточно информации для ответа». \"\n",
    "        f\"Структурируйте свой ответ и отвечайте шаг за шагом, но не упоминайте ссылки на источники в ответе.\"\n",
    "        f\"Проверьте, что ответ выводится на кириллице и на русском языке. \\n\"\n",
    "        f\"ТЕКСТ:\\n{sources_text}\\nВОПРОС:\\n{question}\"\n",
    "    )\n",
    "\n",
    "   \n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "    openai_api_key = \"EMPTY\"\n",
    "    openai_api_base = \"http://localhost:8000/v1\"\n",
    "    \n",
    "    client = OpenAI(\n",
    "        api_key=openai_api_key,\n",
    "        base_url=openai_api_base,\n",
    "    )\n",
    "    \n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "        messages=[\n",
    "            \n",
    "            {\"role\": \"user\", \"content\": promptstring},\n",
    "            \n",
    "        ],\n",
    "        max_tokens = 2048,\n",
    "    )\n",
    "    print(\"Chat response:\", chat_response)\n",
    "\n",
    "    return chat_response.choices[0].message.content, metadatas, rets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fd6db6e-ff82-471f-bf8e-1e1640bf7eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [13358]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://213.171.28.90:8001 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response: ChatCompletion(id='chat-4b2bc72b8d7b45b1820c8d66fa7f11b8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=' Чтобы сменить пароль на видеохостинге RUTUBE, вам необходимо выполнить следующие шаги:\\n\\n1. Авторизуйтесь на сайте RUTUBE (https://rutube.ru).\\n2. В меню профиля найдите пункт \"Изменить пароль\" и нажмите на него.\\n3. Следуйте инструкциям и заполните поле для нового пароля.\\n4. Подтвердите изменение пароля в поле для подтверждения нового пароля.\\n5. Нажмите кнопку \"Сохранить\" или \"Обновить\" для сохранения изменений.\\n\\nВажно обратить внимание, что эти шаги могут отличаться в зависимости от устройства (например, в мобильном приложении будет другой способ изменения пароля). Если у вас возникнут трудности с изменением пароля, я рекомендую пройтись по шагам снова, либо обратиться в службу поддержки RUTUBE.', refusal=None, role='assistant', function_call=None, tool_calls=[]), stop_reason=None)], created=1727592403, model='mistralai/Mistral-7B-Instruct-v0.3', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=301, prompt_tokens=529, total_tokens=830, completion_tokens_details=None), prompt_logprobs=None)\n",
      "INFO:     212.59.108.239:2198 - \"POST /predict HTTP/1.1\" 200 OK\n",
      "INFO:     37.204.156.155:52006 - \"GET /predict HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     37.204.156.155:52006 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n"
     ]
    }
   ],
   "source": [
    "class Request(BaseModel):\n",
    "    question: str\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    answer: str\n",
    "    class_1: str\n",
    "    class_2: str\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "def index():\n",
    "    return {\"text\": \"Интеллектуальный помощник оператора службы поддержки.\"}\n",
    "\n",
    "    \n",
    "@app.post(\"/predict\")\n",
    "async def predict_sentiment(request: Request):\n",
    "    question = request.question\n",
    "  \n",
    "    answer_llm = respond_question(question)\n",
    "  \n",
    "    response = Response(\n",
    "        answer=answer_llm,#anwer_data['Ответ из БЗ'],\n",
    "        class_1='',#anwer_data['Классификатор 1 уровня'], # Классификатор оценивается опционально; при отсутствии можно задать константное значение.\n",
    "        class_2=''#anwer_data['Классификатор 2 уровня'], # Классификатор оценивается опционально; при отсутствии можно задать константное значение.\n",
    "    )\n",
    "    \n",
    "  \n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    host = \"213.171.28.90\" # Сконфигурируйте host согласно настройкам вашего сервера.\n",
    "    config = uvicorn.Config(app, host=host, port=8001)#8000\n",
    "    server = uvicorn.Server(config)\n",
    "    loop = asyncio.get_running_loop()\n",
    "    loop.create_task(server.serve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a732306e-d730-4c92-a610-bc5dd243866e",
   "metadata": {},
   "source": [
    "# Для проверки того, что REST API отвечает корректно, запустите данный код в отдельном ноутбуке или Python-скрипте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444404b6-5df1-43b2-85d6-ba30e4092937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
